#!/bin/bash
#SBATCH --partition=gpup100
#SBATCH --gres=gpu:1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=6
#SBATCH --job-name=4-3
#SBATCH --time=12:0:0
#SBATCH --mail-type=end
#SBATCH --mail-user=dsingh33@jhu.edu
#SBATCH --output=mlm_4_3.log
#SBATCH --error=mlm_4_3.err




module load cuda


module load anaconda
conda activate capstone2


python3 mlm.py \
--dataset_name=c4 \
--cache_dir=cache \
--tokenizer_name=t5-small \
--config_name=configs/tpt-small-discrete-lm \
--model_name_or_path=tpt-small-discrete \
--learning_rate=5e-3 \
--gradient_accumulation_steps=1 \
--train_batch_size=16 \
--num_train_epochs=20 \
--eval_batch_size=32 \
--n_gpu=1 \
--run_id=04 \
--resume_from_epoch=16 \
--resume_ckpt_path=checkpointepoch=15.ckpt \
--scheduler_type=inverse_sqrt \
--output_dir=out \
--overwrite_output_dir \
--optim_name=adafactor \
--fix_lr_step=30000 \
--num_roles_per_layer=50 \
--do_train \
--role_regularization=0.4 \
--seed=42
